{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431df17c-bdaa-460e-9a4e-e8bea56a0ced",
   "metadata": {},
   "source": [
    "# Text CNN Sentiment\n",
    "### 1. import dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca05fd8-430d-418e-8231-249de2f4d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': './data/',\n",
    "    'device_target': 'CPU',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4863c-8333-4cb3-b81f-2977b77aba85",
   "metadata": {},
   "source": [
    "### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f611e689-df8b-4b19-bc7e-bb80afb90cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166498e-bef9-464a-abd9-aea6dbe4211a",
   "metadata": {},
   "source": [
    "### 3. Define data generation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ecd1106-8a88-4505-a6c7-d5944f4506bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list = input_list\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return (np.array(self.input_list[item][0], dtype=np.int32),\n",
    "                np.array(self.input_list[item][1], dtype=np.int32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "# âœ… Step 2: Define MovieReview\n",
    "class MovieReview:\n",
    "    '''Movie review dataset'''\n",
    "    \n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {'neg': 0, 'pos': 1}\n",
    "        self.files = []\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        for root, _, filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root, each))\n",
    "            break\n",
    "\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        \n",
    "        for filename in self.files:\n",
    "            self.read_data(filename)\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "        with open(filePath, 'r') as f:\n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n', '')\\\n",
    "                                .replace('\"', '')\\\n",
    "                                .replace('\\'', '')\\\n",
    "                                .replace('.', '')\\\n",
    "                                .replace(',', '')\\\n",
    "                                .replace('[', '')\\\n",
    "                                .replace(']', '')\\\n",
    "                                .replace('(', '')\\\n",
    "                                .replace(')', '')\\\n",
    "                                .replace(':', '')\\\n",
    "                                .replace('--', '')\\\n",
    "                                .replace('-', ' ')\\\n",
    "                                .replace('\\\\', '')\\\n",
    "                                .replace('0', '')\\\n",
    "                                .replace('1', '')\\\n",
    "                                .replace('2', '')\\\n",
    "                                .replace('3', '')\\\n",
    "                                .replace('4', '')\\\n",
    "                                .replace('5', '')\\\n",
    "                                .replace('6', '')\\\n",
    "                                .replace('7', '')\\\n",
    "                                .replace('8', '')\\\n",
    "                                .replace('9', '')\\\n",
    "                                .replace('`', '')\\\n",
    "                                .replace('=', '')\\\n",
    "                                .replace('$', '')\\\n",
    "                                .replace('/', '')\\\n",
    "                                .replace('*', '')\\\n",
    "                                .replace(';', '')\\\n",
    "                                .replace('<b>', '')\\\n",
    "                                .replace('%', '')\n",
    "\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                \n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    \n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence, self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence, self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        for SentenceLabel in self.Pos + self.Neg:\n",
    "            vector = [0] * maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        trunk_pos_size = math.ceil((1 - split) * len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1 - split) * len(self.Neg))\n",
    "        trunk_num = int(1 / (1 - split))\n",
    "        pos_temp = list()\n",
    "        neg_temp = list()\n",
    "        \n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index * trunk_pos_size:(index + 1) * trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index * trunk_neg_size:(index + 1) * trunk_neg_size])\n",
    "        \n",
    "        self.test = pos_temp.pop(2) + neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp + neg_temp for i in item]\n",
    "        random.shuffle(self.train)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.train), \n",
    "            column_names=[\"data\", \"label\"], \n",
    "            shuffle=False\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        dataset = dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.test), \n",
    "            column_names=[\"data\", \"label\"], \n",
    "            shuffle=False\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f2df18-e739-42ad-9b21-dd3393f486fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b8265d-15f9-4316-b586-9dbf95a5b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[  346,     5,    75 ...     0,     0,     0],\n",
      " [ 5921,     2,    15 ...     0,     0,     0],\n",
      " [   36, 10885,    10 ...     0,     0,     0],\n",
      " ...\n",
      " [   15,   555,  5871 ...     0,     0,     0],\n",
      " [    0,   295,    32 ...     0,     0,     0],\n",
      " [ 6960,  6961,     2 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, \n",
      " 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, \n",
      " 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0])}\n",
      "[5921    2   15 4533  607  323   55  386  273  111  907  210   26 1123\n",
      "  856  155   82    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241f0405-d0d4-4725-aecf-a4216e49f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c30f42-cff7-4d1f-bcde-76cc30f230b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28140695-77fa-460a-8058-3d5cbf9eed08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:12.942.819 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:12.951.807 [mindspore/nn/layer/basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 3.3811075e-03 -7.8216977e-03 -2.3920234e-02 ... -2.6177242e-03\n",
      "         6.9509987e-03  1.3288773e-03]\n",
      "       [-5.2365323e-04  4.1895877e-03  2.0402491e-03 ... -2.0252410e-03\n",
      "         4.4044890e-03 -2.2057421e-03]\n",
      "       [-5.2405000e-03  1.8373411e-02 -4.3608323e-03 ... -8.6876247e-03\n",
      "        -3.2276912e-03  8.1279427e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 3.8855299e-03 -3.0830561e-03  2.6277932e-03 ...  4.8424774e-03\n",
      "        -2.5395368e-04  1.6506327e-02]\n",
      "       [ 8.7999104e-04 -1.1926297e-02  4.6531362e-03 ...  7.2897477e-05\n",
      "        -3.6201673e-03  2.7839473e-02]\n",
      "       [ 8.8969553e-03  6.0449592e-03  4.3209386e-03 ... -1.0683051e-02\n",
      "         2.4460102e-02  1.4625699e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-9.9531431e-03 -6.4906883e-03  1.0225869e-02 ...  1.1382402e-02\n",
      "        -2.6426911e-03 -5.7625417e-03]\n",
      "       [-5.8446256e-03 -8.7942667e-03  1.8017296e-02 ... -5.9945076e-03\n",
      "         5.0160722e-03 -6.1837924e-03]\n",
      "       [ 7.7424040e-03  4.7087735e-03  1.6768444e-03 ... -1.2363949e-02\n",
      "        -1.0361573e-02  1.8321505e-02]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-1.4809330e-02  8.8637723e-03  5.5735535e-03 ...  7.5346171e-03\n",
      "        -2.2114399e-03  3.9350899e-04]\n",
      "       [ 2.7474081e-03 -3.1405993e-04 -2.3644486e-02 ... -1.1992188e-02\n",
      "        -4.4096373e-03  1.5505536e-02]\n",
      "       [ 2.3659935e-02 -4.0671401e-04 -3.3207934e-03 ...  3.8559714e-03\n",
      "        -4.6217958e-03 -5.8952128e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-6.7193448e-03 -6.9220141e-03  1.1080056e-02 ... -1.2224054e-02\n",
      "         8.9571094e-03  5.7208161e-03]\n",
      "       [ 1.9969070e-02  1.0917612e-02  4.9401829e-03 ...  5.0198287e-03\n",
      "         4.4485657e-03 -2.0972337e-03]\n",
      "       [ 8.0951992e-03  3.6537009e-03  1.0255444e-02 ...  2.1027356e-02\n",
      "        -7.8587122e-03  8.8269887e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.7278578e-03 -1.7727520e-02  8.4380340e-03 ...  4.2195269e-03\n",
      "         1.1847718e-02  3.5519549e-03]\n",
      "       [ 1.2187116e-02 -6.3148639e-03  1.0114601e-02 ...  4.9043568e-03\n",
      "         3.0789888e-03  1.2714227e-02]\n",
      "       [ 2.3069259e-05 -9.5026409e-03  1.6568117e-02 ... -1.0511464e-02\n",
      "         4.0802024e-03 -1.1498869e-02]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-9.34448000e-03  2.04006732e-02 -4.56451531e-03 ... -3.58891836e-03\n",
      "        -8.72800779e-03  1.58088151e-02]\n",
      "       [-1.25514334e-02  1.33330235e-02 -1.75211970e-02 ... -3.58615280e-03\n",
      "        -3.59965139e-03 -5.96085005e-03]\n",
      "       [-1.21391946e-02  7.50214979e-03 -1.00403056e-02 ... -3.31818988e-03\n",
      "        -3.30939074e-03 -8.71450268e-03]\n",
      "       [ 1.57054048e-02  8.13033152e-03 -8.11790768e-03 ...  1.13842618e-02\n",
      "        -4.37439943e-04  3.28454794e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.88424066e-02 -2.64978707e-02 -8.07368848e-03 ...  1.22042396e-03\n",
      "        -1.37776546e-02  1.08090171e-03]\n",
      "       [ 7.70076783e-03 -4.30941815e-03 -2.57783122e-05 ...  2.73479568e-03\n",
      "         5.55630447e-03 -5.16139809e-03]\n",
      "       [-3.36115714e-03  5.11048734e-03 -1.57407997e-03 ...  1.23831742e-05\n",
      "         8.38330388e-03 -1.70093980e-02]\n",
      "       [ 1.54920286e-02 -4.86138044e-03  1.95503142e-02 ...  2.48049144e-02\n",
      "         5.84682263e-03  9.00251511e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 3.17261904e-03 -5.34895901e-03  1.05043789e-02 ...  3.61250620e-03\n",
      "        -9.19178710e-04 -1.00191254e-02]\n",
      "       [ 3.85991391e-03 -4.79093799e-03 -1.25133311e-02 ... -1.01680579e-02\n",
      "        -9.40791424e-03 -1.46207400e-02]\n",
      "       [ 1.65730119e-02  2.01018918e-02  7.94976298e-03 ...  5.14351577e-03\n",
      "        -2.86351354e-03  2.52558524e-03]\n",
      "       [-1.76151767e-02 -4.60235216e-03 -4.10656212e-03 ... -1.98557489e-02\n",
      "        -8.82769190e-03  4.70013265e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-7.80048966e-03 -1.81372464e-02  1.06161356e-03 ...  6.62330259e-03\n",
      "        -1.58683537e-03  1.03969257e-02]\n",
      "       [-7.63769029e-03 -9.01857670e-03  7.43874069e-03 ... -2.64960690e-03\n",
      "        -1.41428066e-02  2.56483303e-03]\n",
      "       [-1.50760985e-04 -1.31825292e-02  1.37673747e-02 ...  3.41407582e-03\n",
      "        -9.28617257e-04  6.97344542e-03]\n",
      "       [ 4.77496209e-03  2.39914702e-03  4.22168151e-03 ... -2.66323914e-04\n",
      "         7.00306986e-03  1.20183686e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-9.56132542e-03 -5.76701574e-03 -1.11838169e-02 ... -1.42163336e-02\n",
      "         2.61787412e-04  7.79426564e-03]\n",
      "       [-3.43672512e-03 -4.95689549e-03 -5.42414840e-03 ... -1.35518620e-02\n",
      "         3.50773241e-03  4.80659120e-03]\n",
      "       [-7.63428444e-03  1.84738531e-03 -7.43955490e-04 ... -9.34168138e-03\n",
      "         9.01986845e-03 -1.37534402e-02]\n",
      "       [ 1.82355754e-02 -1.33533515e-02 -1.63499508e-02 ... -2.67902808e-03\n",
      "         9.33586806e-03 -1.60549358e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 9.67601966e-03  9.14891250e-03  2.34156288e-02 ... -3.38943000e-03\n",
      "         2.44061369e-02  1.07765403e-02]\n",
      "       [ 7.25883711e-03  8.61046277e-03 -1.97325516e-02 ...  2.91000796e-03\n",
      "        -6.18674979e-03 -5.79008646e-03]\n",
      "       [ 1.34358287e-03  3.29684187e-03 -1.29549801e-02 ... -3.55091086e-03\n",
      "        -1.66201983e-02  1.60071757e-02]\n",
      "       [ 2.50208639e-02  4.15683782e-04 -1.51272723e-02 ...  2.87871920e-02\n",
      "         7.37844314e-03 -4.42842348e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 2.49166396e-02  1.33823196e-03 -1.45842312e-02 ... -1.42388337e-03\n",
      "         1.40993334e-02  4.11765976e-03]\n",
      "       [ 5.89925284e-03  1.31310234e-02 -1.93687552e-03 ...  1.30191343e-02\n",
      "        -6.16747746e-03 -8.37254059e-03]\n",
      "       [ 2.27271323e-03 -6.16816198e-03 -7.91037828e-03 ... -1.70444488e-04\n",
      "        -7.14484195e-05  1.16035007e-02]\n",
      "       [ 2.14845943e-03 -2.04446958e-03 -3.14548449e-03 ... -4.49185056e-04\n",
      "        -1.39027815e-02 -6.71612623e-04]\n",
      "       [-1.48520840e-03  1.94666500e-03  1.39453001e-02 ... -5.48106059e-03\n",
      "        -2.24642176e-03 -7.26839667e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.04506630e-02  9.86144296e-04  1.21559817e-02 ... -5.75726852e-03\n",
      "         1.11611225e-02  6.16934244e-03]\n",
      "       [ 1.28986510e-02  1.37230591e-03  6.15025731e-03 ... -6.99834898e-04\n",
      "         1.01192296e-02  2.79159262e-03]\n",
      "       [-5.15644252e-03  3.71670537e-03  4.20315890e-03 ... -1.27290227e-02\n",
      "         3.86829488e-03 -1.21317906e-02]\n",
      "       [-1.75155420e-02 -1.20691303e-03  1.47269089e-02 ...  3.28801596e-03\n",
      "        -5.28572174e-03 -2.48163263e-03]\n",
      "       [-6.48510503e-03 -2.89673381e-03 -6.47054939e-03 ... -8.13253224e-03\n",
      "        -1.03262085e-02 -2.50921748e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 5.79585962e-04 -7.29981903e-03  2.84050428e-03 ...  1.39156776e-02\n",
      "        -9.08103865e-03 -1.13452075e-03]\n",
      "       [ 1.45580135e-02  1.18198106e-02  4.06067912e-03 ...  1.45033456e-03\n",
      "         6.99200295e-03 -7.90064223e-03]\n",
      "       [ 1.01722265e-02  6.04812615e-03  5.06684883e-03 ...  5.00720786e-03\n",
      "         7.85777625e-03  3.96037335e-03]\n",
      "       [ 3.71508417e-03 -8.42259824e-03 -1.41119929e-02 ...  1.11333169e-02\n",
      "        -1.07965143e-02  2.23166291e-02]\n",
      "       [ 1.09753273e-02  9.79862921e-03 -5.93078439e-04 ...  6.59235939e-03\n",
      "        -1.46461055e-02  7.99078553e-04]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 1.22498469e-02 -4.79382370e-03 -8.04726966e-03 ...  1.11539066e-02\n",
      "        -3.41495685e-02  1.63141619e-02]\n",
      "       [ 6.02535531e-03  6.36696257e-03 -3.87815339e-03 ...  6.44420832e-03\n",
      "        -5.42744470e-04  4.25942661e-03]\n",
      "       [-9.12522338e-03  7.96039216e-03  1.06301215e-02 ...  1.21181225e-02\n",
      "         1.31610548e-02  1.68106016e-02]\n",
      "       [-4.29005502e-03 -1.01838335e-02 -9.42268889e-05 ... -2.52245925e-03\n",
      "        -9.46961995e-03 -1.33873627e-03]\n",
      "       [ 1.67558249e-02  2.78972946e-02  9.83520690e-03 ... -5.70554403e-04\n",
      "         1.85070205e-02 -1.00498330e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-6.18866365e-03  7.13850558e-03 -5.38109522e-03 ... -8.52091587e-04\n",
      "        -2.28882511e-03 -5.15456544e-03]\n",
      "       [ 2.30324510e-02 -8.39594752e-03  4.77250712e-03 ... -6.71349140e-03\n",
      "        -8.86447262e-03 -1.17272383e-03]\n",
      "       [-5.32374904e-03 -1.68217940e-03 -6.98394142e-03 ... -1.26017416e-02\n",
      "         4.28250292e-03 -2.89096474e-03]\n",
      "       [-3.49887484e-03  1.72278343e-03  1.48510253e-02 ... -1.01160081e-02\n",
      "         7.35251466e-04 -1.68080762e-04]\n",
      "       [ 1.69208050e-02  1.07070906e-02  3.61038023e-03 ...  9.24328808e-03\n",
      "         6.83194818e-03  8.50447919e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.13434531e-02  3.91094625e-04  9.64165758e-03 ...  8.94101895e-03\n",
      "         3.30015428e-05  7.65492348e-03]\n",
      "       [-7.42597785e-03 -3.54797835e-03 -7.90749397e-03 ... -1.43122487e-02\n",
      "        -1.00472867e-02 -1.84089094e-02]\n",
      "       [-1.06300078e-02  1.59505345e-02 -1.36557501e-02 ...  5.24147123e-04\n",
      "         4.64124605e-03  3.02133197e-03]\n",
      "       [-3.27524240e-03  1.01601041e-03  4.57990682e-03 ...  1.21427653e-02\n",
      "         4.82590077e-03  2.12500039e-02]\n",
      "       [-1.78113021e-02 -9.94309084e-04 -4.09970526e-03 ... -4.14263038e-03\n",
      "        -9.63070709e-03  6.13105344e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4b043-06eb-49f1-9e08-f8bb7884da04",
   "metadata": {},
   "source": [
    "### Adding optimizer and Loss Function for time monitor settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8565ce-740e-4674-a7ff-dcbffc02aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339efac1-9433-4d52-ab8d-5f0b6f1a5646",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f086a58-e879-4ac5-a4f9-ea6e4f35f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:12.999.665 [mindspore/train/model.py:1084] For TrainingFinished callback, {'end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:13.251.51 [mindspore/nn/layer/basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:13.467.91 [mindspore/nn/layer/basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:13.831.37 [mindspore/nn/layer/basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:13.931.78 [mindspore/nn/layer/basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(8303:140473846978368,MainProcess):2026-02-15-23:09:13.179.146 [mindspore/nn/layer/basic.py:193] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 596, loss is 0.08073988556861877\n",
      "Train epoch time: 24994.135 ms, per step time: 41.936 ms\n",
      "epoch: 2 step: 596, loss is 0.009903096593916416\n",
      "Train epoch time: 22790.496 ms, per step time: 38.239 ms\n",
      "epoch: 3 step: 596, loss is 0.0036247996613383293\n",
      "Train epoch time: 22624.412 ms, per step time: 37.960 ms\n",
      "epoch: 4 step: 596, loss is 0.0032868029084056616\n",
      "Train epoch time: 23280.874 ms, per step time: 39.062 ms\n",
      "\n",
      "ðŸŽ‰ Training Completed Successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mindspore.train.callback import LossMonitor, TimeMonitor, Callback\n",
    "\n",
    "dataset_size = dataset.get_dataset_size()\n",
    "\n",
    "time_cb = TimeMonitor(data_size=dataset_size)\n",
    "loss_cb = LossMonitor(per_print_times=dataset_size)\n",
    "\n",
    "class TrainingFinished(Callback):\n",
    "    def end(self, run_context):\n",
    "        print(\"\\nðŸŽ‰ Training Completed Successfully!\\n\")\n",
    "\n",
    "finish_cb = TrainingFinished()\n",
    "\n",
    "model.train(\n",
    "    cfg.epoch_size,\n",
    "    dataset,\n",
    "    callbacks=[time_cb, loss_cb, finish_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913c4e5-5dd4-4d8a-a52a-63a603c9efe8",
   "metadata": {},
   "source": [
    "### Testing my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd705f24-fbda-46fb-aa15-9db4e016bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"The word does not appear in the dictionary.\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1979c-ec8a-4add-9727-57b469f8858a",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69761029-7db7-4d79-bf8a-c73dd3a23c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is so Cool but some how enjoyable \"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1f0f6-5239-4bb9-af55-d3cddaad359c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
