{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431df17c-bdaa-460e-9a4e-e8bea56a0ced",
   "metadata": {},
   "source": [
    "# Text CNN Sentiment\n",
    "### 1. import dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca05fd8-430d-418e-8231-249de2f4d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': './data/',\n",
    "    'device_target': 'CPU',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4863c-8333-4cb3-b81f-2977b77aba85",
   "metadata": {},
   "source": [
    "### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f611e689-df8b-4b19-bc7e-bb80afb90cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166498e-bef9-464a-abd9-aea6dbe4211a",
   "metadata": {},
   "source": [
    "### 3. Define data generation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ecd1106-8a88-4505-a6c7-d5944f4506bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list = input_list\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return (np.array(self.input_list[item][0], dtype=np.int32),\n",
    "                np.array(self.input_list[item][1], dtype=np.int32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "# âœ… Step 2: Define MovieReview\n",
    "class MovieReview:\n",
    "    '''Movie review dataset'''\n",
    "    \n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {'neg': 0, 'pos': 1}\n",
    "        self.files = []\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        for root, _, filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root, each))\n",
    "            break\n",
    "\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        \n",
    "        for filename in self.files:\n",
    "            self.read_data(filename)\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "        with open(filePath, 'r') as f:\n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n', '')\\\n",
    "                                .replace('\"', '')\\\n",
    "                                .replace('\\'', '')\\\n",
    "                                .replace('.', '')\\\n",
    "                                .replace(',', '')\\\n",
    "                                .replace('[', '')\\\n",
    "                                .replace(']', '')\\\n",
    "                                .replace('(', '')\\\n",
    "                                .replace(')', '')\\\n",
    "                                .replace(':', '')\\\n",
    "                                .replace('--', '')\\\n",
    "                                .replace('-', ' ')\\\n",
    "                                .replace('\\\\', '')\\\n",
    "                                .replace('0', '')\\\n",
    "                                .replace('1', '')\\\n",
    "                                .replace('2', '')\\\n",
    "                                .replace('3', '')\\\n",
    "                                .replace('4', '')\\\n",
    "                                .replace('5', '')\\\n",
    "                                .replace('6', '')\\\n",
    "                                .replace('7', '')\\\n",
    "                                .replace('8', '')\\\n",
    "                                .replace('9', '')\\\n",
    "                                .replace('`', '')\\\n",
    "                                .replace('=', '')\\\n",
    "                                .replace('$', '')\\\n",
    "                                .replace('/', '')\\\n",
    "                                .replace('*', '')\\\n",
    "                                .replace(';', '')\\\n",
    "                                .replace('<b>', '')\\\n",
    "                                .replace('%', '')\n",
    "\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                \n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    \n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence, self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence, self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        for SentenceLabel in self.Pos + self.Neg:\n",
    "            vector = [0] * maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        trunk_pos_size = math.ceil((1 - split) * len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1 - split) * len(self.Neg))\n",
    "        trunk_num = int(1 / (1 - split))\n",
    "        pos_temp = list()\n",
    "        neg_temp = list()\n",
    "        \n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index * trunk_pos_size:(index + 1) * trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index * trunk_neg_size:(index + 1) * trunk_neg_size])\n",
    "        \n",
    "        self.test = pos_temp.pop(2) + neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp + neg_temp for i in item]\n",
    "        random.shuffle(self.train)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.train), \n",
    "            column_names=[\"data\", \"label\"], \n",
    "            shuffle=False\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        dataset = dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.test), \n",
    "            column_names=[\"data\", \"label\"], \n",
    "            shuffle=False\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f2df18-e739-42ad-9b21-dd3393f486fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b8265d-15f9-4316-b586-9dbf95a5b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[ 1090,   411,  1329 ...     0,     0,     0],\n",
      " [  339,   508,   509 ...     0,     0,     0],\n",
      " [15496, 15497,  2049 ...     0,     0,     0],\n",
      " ...\n",
      " [  128,    15, 17940 ...     0,     0,     0],\n",
      " [ 5970, 13894,  3470 ...     0,     0,     0],\n",
      " [ 2031,   160,    90 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, \n",
      " 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, \n",
      " 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1])}\n",
      "[ 339  508  509 2169  152    4   14   32   27 2170 1865 2171  359   11\n",
      " 2172 2173 2054  253 2174  180   32   82    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241f0405-d0d4-4725-aecf-a4216e49f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
